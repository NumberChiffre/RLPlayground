{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "pycharm": {}
      },
      "outputs": [],
      "source": "import ray\nimport json\nfrom _jsonnet import evaluate_file\nimport numpy as np\nimport os\n\nROOT_DIR \u003d os.path.dirname(os.path.abspath(__file__))\n# ROOT_DIR \u003d \u0027/content/drive/My Drive/Colab Notebooks\u0027\nRESULT_DIR \u003d f\u0027{ROOT_DIR}/results\u0027\nCONFIG_DIR \u003d f\u0027{ROOT_DIR}/configs\u0027\nTENSORBOARD_DIR \u003d f\u0027{ROOT_DIR}/tensorboards\u0027\n\n\"\"\"\n:class:`~data_pulling.utilities.registrable.Registrable` is a \"mixin\" for endowing\nany base class with a named registry for its subclasses and a decorator\nfor registering them. It is adapted from the allennlp codebase:\nhttps://github.com/allenai/allennlp/blob/master/allennlp/common/registrable.py\n\"\"\"\n\nimport logging\nfrom collections import defaultdict\nfrom typing import TypeVar, Type, Dict, List\n\nlogger \u003d logging.getLogger(__name__)\n\nT \u003d TypeVar(\u0027T\u0027)\n\n\nclass Registrable:\n    \"\"\"\n    Any class that inherits from ``Registrable`` gains access to a named\n    registry for its subclasses. To register them, just decorate them with the\n    classmethod ``@BaseClass.register(name)``.\n    After which you can call ``BaseClass.list_available()`` to get the keys for\n    the registered subclasses, and ``BaseClass.by_name(name)`` to get the\n    corresponding subclass.\n\n    Note that the registry stores the subclasses themselves; not class\n    instances. In most cases you would then call ``from_params(params)`` on the\n    returned subclass.\n\n    You can specify a default by setting ``BaseClass.default_implementation``.\n    If it is set, it will be the first element of ``list_available()``.\n    Note that if you use this class to implement a new ``Registrable`` abstract\n    class, you must ensure that all subclasses of the abstract class are loaded\n    when the module is loaded, because the subclasses register themselves in\n    their respective files. You can achieve this by having the abstract class\n    and all subclasses in the __init__.py of the module in which they reside\n    (as this causes any import of either the abstract class or a subclass to\n    load all other subclasses and the abstract class).\n    \"\"\"\n    _registry: Dict[Type, Dict[str, Type]] \u003d defaultdict(dict)\n    default_implementation: str \u003d None\n\n    @classmethod\n    def register(cls: Type[T], name: str):\n        registry \u003d Registrable._registry[cls]\n\n        def add_subclass_to_registry(subclass: Type[T]):\n            # Add to registry, raise an error if key has already been used.\n            if name in registry:\n                message \u003d \"Cannot register %s as %s; name already in use for %s\" % (\n                    name, cls.__name__, registry[name].__name__)\n                raise ConfigurationError(message)\n            registry[name] \u003d subclass\n            return subclass\n\n        return add_subclass_to_registry\n\n    @classmethod\n    def by_name(cls: Type[T], name: str) -\u003e Type[T]:\n        logger.debug(f\"instantiating registered subclass {name} of {cls}\")\n        if name not in Registrable._registry[cls]:\n            raise ConfigurationError(\n                \"%s is not a registered name for %s\" % (name, cls.__name__))\n        return Registrable._registry[cls].get(name)\n\n    @classmethod\n    def list_available(cls) -\u003e List[str]:\n        \"\"\"List default first if it exists\"\"\"\n        keys \u003d list(Registrable._registry[cls].keys())\n        default \u003d cls.default_implementation\n\n        if default is None:\n            return keys\n        elif default not in keys:\n            message \u003d \"Default implementation %s is not registered\" % default\n            raise ConfigurationError(message)\n        else:\n            return [default] + [k for k in keys if k !\u003d default]\n\n\nclass ConfigurationError(Exception):\n    def __init__(self, message):\n        super(ConfigurationError, self).__init__()\n        self.message \u003d message\n\n    def __str__(self):\n        return repr(self.message)\n\n\nimport numpy as np\nfrom enum import Enum\nfrom typing import NamedTuple, Union\n\n\nclass RLAlgorithm(Enum):\n    VI \u003d \u0027value_iteration\u0027\n    PI \u003d \u0027policy_iteration\u0027\n    QLearning \u003d \u0027q_learning\u0027\n    MONTE_CARLO \u003d \u0027monte_carlo\u0027\n    EXPECTED_SARSA \u003d \u0027expected_sarsa\u0027\n    SARSA \u003d \u0027sarsa\u0027\n\n\nclass TargetUpdate(Enum):\n    HARD \u003d \u0027hard\u0027\n    SOFT \u003d \u0027soft\u0027\n\n\nclass ReplayType(Enum):\n    EXPERIENCE_REPLAY \u003d \u0027ExperienceReplay\u0027\n    PRIORITIZED_EXPERIENCE_REPLAY \u003d \u0027PrioritizedExperienceReplay\u0027\n\n\nclass Transition(NamedTuple):\n    # store state, action, reward, next_state, done as Transition tuple\n    s0: np.ndarray\n    a: Union[int, str]  # Action\n    r: np.ndarray\n    s1: np.ndarray\n    done: bool \u003d False\n\n\n\nimport torch\nfrom collections import defaultdict\nfrom torch.distributions import Categorical\nimport numpy as np\nfrom gym import Env\n\n\ndef nested_d():\n    \"\"\"for any arbitrary number of levels\"\"\"\n    return defaultdict(nested_d)\n\n\n# Thanks Vlad!\ndef torch_argmax_mask(q: torch.Tensor, dim: int):\n    \"\"\" Returns a random tie-breaking argmax mask\n    Example:\n        \u003e\u003e\u003e import torch\n        \u003e\u003e\u003e torch.manual_seed(1337)\n        \u003e\u003e\u003e q \u003d torch.ones(3, 2)\n        \u003e\u003e\u003e torch_argmax_mask(q, 1)\n        # tensor([[False,  True],\n        #         [ True, False],\n        #         [ True, False]])\n        \u003e\u003e\u003e torch_argmax_mask(q, 1)\n        # tensor([[False,  True],\n        #         [False,  True],\n        #         [ True, False]])\n    \"\"\"\n    rand \u003d torch.rand_like(q)\n    if dim \u003d\u003d 0:\n        mask \u003d rand * (q \u003d\u003d q.max(dim)[0])\n        mask \u003d mask \u003d\u003d mask.max(dim)[0]\n        assert int(mask.sum()) \u003d\u003d len(q.shape)\n    elif dim \u003d\u003d 1:\n        mask \u003d rand * (q \u003d\u003d q.max(dim)[0].unsqueeze(1).expand(q.shape))\n        mask \u003d mask \u003d\u003d mask.max(dim)[0].unsqueeze(1).expand(q.shape)\n        assert int(mask.sum()) \u003d\u003d int(q.shape[0])\n    else:\n        raise NotImplemented(\"Only vectors and matrices are supported\")\n    return mask\n\n\ndef get_epsilon_dist(eps: float, env: Env, observation: torch.Tensor,\n                     model: torch.nn.Module) -\u003e Categorical:\n    \"\"\"get the probability distributions of the q-value\"\"\"\n    q \u003d model(observation)\n    probs \u003d torch.empty_like(q).fill_(\n        eps / (env.action_space.n - 1))\n    probs[torch_argmax_mask(q, len(q.shape) - 1)] \u003d 1 - eps\n    return Categorical(probs\u003dprobs)\n\n\ndef get_epsilon(eps_start: float, eps_final: float, eps_decay: float,\n                t: int) -\u003e float:\n    \"\"\"use decay for epsilon exploration\"\"\"\n    return eps_final + (eps_start - eps_final) * np.exp(-1.0 * t / eps_decay)\n\n\ndef soft_update(value_net: torch.nn.Module, target_net: torch.nn.Module,\n                tau: float):\n    \"\"\"update each training step by a hyperparameter adjustment\"\"\"\n    for t_param, v_param in zip(target_net.parameters(),\n                                value_net.parameters()):\n        if t_param is v_param:\n            continue\n        new_param \u003d tau * v_param.data + (1.0 - tau) * t_param.data\n        t_param.data.copy_(new_param)\n\n\ndef hard_update(value_net: torch.nn.Module, target_net: torch.nn.Module):\n    \"\"\"update each training step by a full update, based on an update frequency\"\"\"\n    for t_param, v_param in zip(target_net.parameters(),\n                                value_net.parameters()):\n        if t_param is v_param:\n            continue\n        new_param \u003d v_param.data\n        t_param.data.copy_(new_param)\n\n\nimport logging\nfrom typing import List\nfrom termcolor import colored\n\n\nclass ProjectLogger:\n    def __init__(self,\n                 log_file: str \u003d None,\n                 level: int \u003d logging.DEBUG,\n                 printing: bool \u003d True, attrs: List[str] \u003d None,\n                 name: str \u003d \u0027project_logger\u0027,\n                 ):\n        \"\"\" Basic logger that can write to a file on disk or to sterr.\n        :param log_file: name of the file to log to\n        :param level: logging verbosity level\n        :param printing: flag for whether to log to sterr\n        \"\"\"\n        root_logger \u003d logging.getLogger(name)\n        root_logger.setLevel(level)\n        self.printing \u003d printing\n        self.attrs \u003d attrs\n\n        # Set up writing to a file\n        if log_file:\n            file_handler \u003d logging.FileHandler(log_file, mode\u003d\u0027a\u0027)\n            file_formatter \u003d logging.Formatter(\n                \u0027%(levelname)s: %(asctime)s %(message)s\u0027,\n                datefmt\u003d\u0027%m/%d/%Y %image:%M:%S %p\u0027\n            )\n            file_handler.setFormatter(file_formatter)\n            root_logger.addHandler(file_handler)\n\n        # Set up printing to stderr\n        def check_if_sterr(hdlr: logging.Handler):\n            return isinstance(hdlr, logging.StreamHandler) \\\n                   and not isinstance(hdlr, logging.FileHandler)\n\n        if printing and not list(filter(check_if_sterr, root_logger.handlers)):\n            console_handler \u003d logging.StreamHandler()\n            console_handler.setFormatter(logging.Formatter(\"%(message)s\"))\n            root_logger.addHandler(console_handler)\n\n        self.log \u003d root_logger\n\n    def debug(self, msg, color\u003d\u0027grey\u0027, attrs: List[str] \u003d None):\n        self.log.debug(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def info(self, msg, color\u003d\u0027green\u0027, attrs: List[str] \u003d None):\n        self.log.info(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def warning(self, msg, color\u003d\u0027blue\u0027, attrs: List[str] \u003d None):\n        self.log.warning(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def error(self, msg, color\u003d\u0027magenta\u0027, attrs: List[str] \u003d None):\n        self.log.error(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n    def critical(self, msg, color\u003d\u0027red\u0027, attrs: List[str] \u003d None):\n        self.log.critical(colored(msg, color, attrs\u003dattrs or self.attrs))\n\n\nimport pickle\nimport os\nfrom typing import Dict\nfrom collections import defaultdict\nimport plotly\nimport plotly.graph_objects as go\nfrom torch.utils.tensorboard import SummaryWriter\n\n\ndef plot_episodic_results(idx: int, seed: int, writer: SummaryWriter,\n                          episode_result: defaultdict):\n    \"\"\"writes episodic results into tensorboard, called at the end of each\n    episode\"\"\"\n    for k, v in episode_result.items():\n        for i in range(idx, idx + len(v)):\n            if \u0027net_params\u0027 not in k:\n                writer.add_scalar(tag\u003dk, scalar_value\u003dv[i - idx], global_step\u003di)\n            else:\n                for tag, value in v[i - idx]:\n                    tag_ \u003d f\"{tag.replace(\u0027.\u0027, \u0027/\u0027)}/{str(seed)}\"\n                    writer.add_histogram(tag_, value.data.cpu().numpy(), i)\n                    tag_ \u003d f\"{tag.replace(\u0027.\u0027, \u0027/\u0027)}/grad/{str(seed)}\"\n                    writer.add_histogram(tag_, value.grad.data.cpu().numpy(),\n                                         i)\n\n\n\nimport random\nfrom collections import deque\nfrom typing import List, Dict, Tuple\nimport torch\nimport numpy as np\n\nclass Replay(Registrable):\n    @classmethod\n    def build(cls, type: str, params: Dict):\n        replay \u003d cls.by_name(type)\n        return replay.from_params(params)\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        raise NotImplementedError(\n            f\u0027from_params not implemented in {cls.__class__.name}\u0027)\n\n\n# TODO: initial version, not optimized with tree structures used in paper\n@Replay.register(\u0027ExperienceReplay\u0027)\nclass ExperienceReplay(Replay, Registrable):\n    def __init__(self,\n                 capacity: int,\n                 n_step: int,\n                 gamma: float):\n        \"\"\"\n\n        :param capacity: maximum number of transition tuple stored in replay\n        :param n_step: n step used for replay\n        :param gamma: discount factor when computing td-error\n        \"\"\"\n        self.replay_type \u003d self.__class__.__name__\n        self.capacity \u003d capacity\n        self.memory \u003d deque(maxlen\u003dself.capacity)\n        self.n_step \u003d n_step\n        if self.n_step \u003e 0:\n            self.n_step_memory \u003d deque(maxlen\u003dself.n_step)\n            self.gamma \u003d gamma\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def push(self, transition: Transition):\n        \"\"\"push Transition into memory for batch sampling and n_step\n        computations\"\"\"\n        if self.n_step \u003e 0:\n            self.n_step_memory.append(transition)\n            if len(self.n_step_memory) \u003d\u003d self.n_step:\n                transition \u003d self.generate_n_step_q()\n        self.memory.append(transition)\n\n    # TODO: try running with this update\n    def generate_n_step_q(self) -\u003e Transition:\n        \"\"\"with s(t), s(t+1), calculate a discounted reward by backtracking\n        n_steps prior to t and setting s(t) to s(t-n_steps)\"\"\"\n        transitions \u003d self.n_step_memory\n        reward \u003d 0\n        next_observation, done \u003d transitions[-1][-2:]\n        for idx, transition in enumerate(transitions):\n            reward +\u003d (self.gamma ** idx) * transition.r\n        # for i in range(len(transitions) - 1):\n        #     reward \u003d self.gamma * reward * (1 - transitions[i].done) + \\\n        #              transitions[i].r\n        #     next_observation, done \u003d (transitions[i].s1, transitions[i].done) \\\n        #         if transitions[i].done else (next_observation, done)\n        observation, action \u003d transitions[0][:2]\n        return Transition(s0\u003dobservation, a\u003daction, r\u003dreward,\n                          s1\u003dnext_observation, done\u003ddone)\n\n    def sample(self, batch_size: int) -\u003e List[Transition]:\n        \"\"\"Uniform sampling with a batch from memory and concatenates the\n        dimensions of observations and convert to torch\"\"\"\n        batch \u003d random.sample(self.memory, batch_size if len(\n            self.memory) \u003e batch_size else len(self.memory))\n        observation, action, reward, next_observation, done \u003d zip(*batch)\n        observation \u003d torch.cat(tuple(torch.FloatTensor(observation)), dim\u003d0)\n        action \u003d torch.LongTensor(action)\n        reward \u003d torch.FloatTensor(reward)\n        next_observation \u003d torch.cat(tuple(torch.FloatTensor(next_observation)),\n                                     dim\u003d0)\n        done \u003d torch.FloatTensor(done)\n        return Transition(s0\u003dobservation, a\u003daction, r\u003dreward,\n                          s1\u003dnext_observation, done\u003ddone)\n\n    def __len__(self):\n        return len(self.memory)\n\n\n@Replay.register(\u0027PrioritizedExperienceReplay\u0027)\nclass PrioritizedExperienceReplay(ExperienceReplay, Registrable):\n    def __init__(self,\n                 capacity: int,\n                 n_step: int,\n                 alpha: float,\n                 beta: float,\n                 beta_inc: float,\n                 gamma: float,\n                 non_zero_variant: float):\n\n        \"\"\"\n\n        :param capacity: maximum number of transition tuple stored in replay\n        :param n_step: n step used for replay\n        :param gamma: discount rate for calculating td-error\n        :param alpha: 0 for no prioritization, 1 for full prioritization\n        :param beta:\n        :param beta_inc:\n        :param non_zero_variant: small constant to ensure non-zero probabilities\n        \"\"\"\n        # try with alpha\u003d0.6, beta\u003d0.4, beta_inc\u003d100~network update frequency\n        super().__init__(capacity\u003dcapacity, n_step\u003dn_step, gamma\u003dgamma)\n        assert alpha + beta \u003d\u003d 1.0\n        self.alpha \u003d alpha\n        self.beta \u003d beta\n        self.beta_inc \u003d (1 - beta) / beta_inc\n        self.non_zero_variant \u003d non_zero_variant\n        self.priorities \u003d np.zeros([self.capacity], dtype\u003dnp.float32)\n        self.idx \u003d 0\n        self.memory \u003d []\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def push(self, transition: Transition):\n        max_prior \u003d np.max(self.priorities) if self.memory else 1.0\n\n        # n_step computation\n        if self.n_step \u003e 0:\n            self.n_step_memory.append(transition)\n            if len(self.n_step_memory) \u003d\u003d self.n_step:\n                transition \u003d self.generate_n_step_q()\n\n        # unlike ExperienceReplay which updates based on FIFO\n        # update from start of queue\n        if len(self.memory) \u003c self.capacity:\n            self.memory.append(transition)\n        else:\n            self.memory[self.idx] \u003d transition\n        self.priorities[self.idx] \u003d max_prior\n        self.idx +\u003d 1\n        self.idx \u003d self.idx % self.capacity\n\n    def sample(self, batch_size: int) -\u003e Tuple[List[Transition], np.array]:\n        \"\"\"use absolute td-error to favor model to optimize\"\"\"\n        if len(self.memory) \u003c self.capacity:\n            probs \u003d self.priorities[:len(self.memory)]\n        else:\n            probs \u003d self.priorities\n        # probs \u003d abs(td-error), use probabilities\n        probs \u003d (probs ** self.alpha) / np.sum(probs ** self.alpha)\n        self.indices \u003d np.random.choice(len(self.memory), batch_size, p\u003dprobs)\n        if self.beta \u003c 1:\n            self.beta +\u003d self.beta_inc\n\n        # samples a batch from memory and concatenates the dimensions of\n        # observations and convert to torch\n        batch \u003d [self.memory[idx] for idx in self.indices]\n        observation, action, reward, next_observation, done \u003d zip(*batch)\n        observation \u003d torch.cat(tuple(torch.FloatTensor(observation)), dim\u003d0)\n        action \u003d torch.LongTensor(action)\n        reward \u003d torch.FloatTensor(reward)\n        next_observation \u003d torch.cat(tuple(torch.FloatTensor(next_observation)),\n                                     dim\u003d0)\n        done \u003d torch.FloatTensor(done)\n\n        # need weights to compute loss\n        weights \u003d (len(self.memory) * probs[self.indices]) ** -self.beta\n        weights \u003d np.array(weights / np.max(weights), dtype\u003dnp.float32)\n        return Transition(s0\u003dobservation, a\u003daction, r\u003dreward,\n                          s1\u003dnext_observation, done\u003ddone), weights\n\n    def update_priorities(self, losses: np.array):\n        \"\"\"update absolute td-error to compute probabilities\"\"\"\n        for idx, priority in zip(self.indices, losses):\n            self.priorities[idx] \u003d priority\n\n\n\nfrom typing import Dict, Generator, List, Tuple\nimport torch\nimport torch.nn as nn\n\n\nclass TorchModel(nn.Module, Registrable):\n    @classmethod\n    def build(cls, type: str, params: Dict):\n        model \u003d cls.by_name(type)\n        return model.from_params(params)\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def forward(self, *input):\n        raise NotImplementedError()\n\n    def to_device(self, device):\n        self.to(device)\n        self.device \u003d device\n\n\n@TorchModel.register(\u0027LinearFCBody\u0027)\nclass LinearFCBody(TorchModel):\n    def __init__(self,\n                 seed: int,\n                 state_dim: int,\n                 action_dim: int,\n                 hidden_units: List \u003d [64, 64],\n                 gate: nn.ReLU \u003d nn.ReLU):\n        super(LinearFCBody, self).__init__()\n        self.seed \u003d torch.manual_seed(seed)\n        self.state_dim \u003d state_dim\n        self.action_dim \u003d action_dim\n        hidden_unit_1, hidden_unit_2 \u003d hidden_units[0], hidden_units[1]\n        self.fc1 \u003d nn.Sequential(\n            nn.Linear(self.state_dim, hidden_unit_1),\n            # nn.BatchNorm1d(hidden_size1),\n            gate()\n        )\n        self.fc2 \u003d nn.Sequential(\n            nn.Linear(hidden_unit_1, hidden_unit_2),\n            # nn.BatchNorm1d(hidden_size2),\n            gate()\n        )\n        self.fc3 \u003d nn.Linear(hidden_unit_2, self.action_dim)\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def forward(self, x):\n        x \u003d self.fc1(x)\n        x \u003d self.fc2(x)\n        x \u003d self.fc3(x)\n        return x\n\n\n@TorchModel.register(\u0027BasicRNN\u0027)\nclass BasicRNN(TorchModel):\n    def __init__(self,\n                 seed: int,\n                 input_dim: int,\n                 hidden_dim: int,\n                 num_layers: int,\n                 output_dim: int,\n                 dropout: float \u003d 0,\n                 bidirectional: bool \u003d False):\n        super().__init__()\n        self.seed \u003d torch.manual_seed(seed)\n        self.hidden_dim \u003d hidden_dim\n        self.num_layers \u003d num_layers\n        self.num_directions \u003d 2 if bidirectional else 1\n        self.lstm \u003d nn.LSTM(input_dim, hidden_dim, num_layers, batch_first\u003dTrue,\n                            bidirectional\u003dbidirectional)\n        self.fc \u003d nn.Linear(hidden_dim * self.num_directions, output_dim)\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def forward(self, x):\n        # Set initial hidden and cell states\n        h0 \u003d torch.zeros(self.num_layers * self.num_directions,\n                         x.size(0), self.hidden_dim).to(self.device)\n        c0 \u003d torch.zeros(self.num_layers * self.num_directions,\n                         x.size(0), self.hidden_dim).to(self.device)\n\n        # Forward propagate LSTM\n        # out: tensor of shape (batch_size, seq_length, hidden_dim)\n        out, _ \u003d self.lstm(x, (h0, c0))\n        out \u003d self.fc(out[:, -1, :])\n        return out\n\n\n@TorchModel.register(\u0027Inception\u0027)\nclass Inception(TorchModel):\n    def __init__(self,\n                 in_channels: int,\n                 gate: nn.ReLU):\n        super().__init__()\n        out_channels \u003d 32\n        self.conv1 \u003d nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size\u003d(1, 1)),\n            # gate(),\n            nn.Conv2d(out_channels, out_channels, kernel_size\u003d(3, 1)), )\n        self.conv2 \u003d nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size\u003d(1, 1)),\n            # gate(),\n            nn.Conv2d(out_channels, out_channels, kernel_size\u003d(5, 1)), )\n        self.conv3 \u003d nn.Sequential(\n            nn.MaxPool2d(kernel_size\u003d(3, 1)),\n            # gate(),\n            nn.Conv2d(in_channels, out_channels, kernel_size\u003d(1, 1)), )\n\n    @classmethod\n    def from_params(cls, params: Dict):\n        return cls(**params)\n\n    def forward(self, x):\n        # (batch_size, out_channels, timesteps, features),\n        # timesteps\u003dtimesteps-kernel+1\n        conv1 \u003d self.conv1(x)\n        conv2 \u003d self.conv2(x)\n        conv3 \u003d self.conv3(x)\n        outputs \u003d [conv1, conv2, conv3]\n        return torch.cat(outputs, 2)  # concatenate by timesteps\n\n\n\nfrom gym import Env\nfrom typing import Dict\n\n\nclass Agent(Registrable):\n    @classmethod\n    def build(cls, type: str, env: Env, params: Dict):\n        agent \u003d cls.by_name(type)\n        return agent.from_params(env, params)\n\n    @classmethod\n    def from_params(cls, env: Env, params: Dict):\n        raise NotImplementedError(\n            f\u0027from_params not implemented in {cls.__name__}\u0027)\n\n\nimport time\nimport numpy as np\nfrom typing import Dict, Generator, Tuple\nfrom gym import Env\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch\n# import torchviz\n\n\n@Agent.register(\u0027DeepTDAgent\u0027)\nclass DeepTDAgent(Agent, Registrable):\n    def __init__(self,\n                 env: Env,\n                 agent_cfg: Dict):\n        \"\"\"\n\n        :param env: gym environment used for Experiment\n        :param agent_cfg: config file for given agent\n        \"\"\"\n        super().__init__()\n        self.env \u003d env\n        self.epochs, self.total_steps \u003d 0, 0\n        self.episodic_result \u003d dict()\n        self.episodic_result[\u0027Training/Q-Loss\u0027] \u003d []\n        self.episodic_result[\u0027Training/Mean-Q-Value-Action\u0027] \u003d []\n        self.episodic_result[\u0027Training/Mean-Q-Value-Opposite-Action\u0027] \u003d []\n        self.episodic_result[\u0027value_net_params\u0027] \u003d []\n\n        # specs for RL agent\n        self.eps \u003d agent_cfg[\u0027eps\u0027]\n        if agent_cfg[\u0027use_eps_decay\u0027]:\n            self.use_eps_decay \u003d agent_cfg[\u0027use_eps_decay\u0027]\n            self.eps_decay \u003d agent_cfg[\u0027eps_decay\u0027]\n            self.eps_min \u003d agent_cfg[\u0027eps_min\u0027]\n        self.gamma \u003d agent_cfg[\u0027gamma\u0027]\n        self.update_type \u003d agent_cfg[\u0027update_type\u0027]\n        if self.update_type \u003d\u003d TargetUpdate.SOFT.value:\n            self.tau \u003d agent_cfg[\u0027tau\u0027]\n        self.update_freq \u003d agent_cfg[\u0027update_freq\u0027]\n        self.warm_up_freq \u003d agent_cfg[\u0027warm_up_freq\u0027]\n        self.use_grad_clipping \u003d agent_cfg[\u0027use_grad_clipping\u0027]\n        self.grad_clipping \u003d agent_cfg[\u0027grad_clipping\u0027]\n        self.lr \u003d agent_cfg[\u0027lr\u0027]\n        self.batch_size \u003d agent_cfg[\u0027batch_size\u0027]\n        self.seed \u003d agent_cfg[\u0027seed\u0027]\n        self.params \u003d vars(self).copy()\n\n        # details experience replay\n        self.replay_buffer \u003d Replay.build(\n            type\u003dagent_cfg[\u0027experience_replay\u0027][\u0027type\u0027],\n            params\u003dagent_cfg[\u0027experience_replay\u0027][\u0027params\u0027])\n\n        # details for the NN model\n        agent_cfg[\u0027model\u0027][\u0027seed\u0027] \u003d self.seed\n        use_cuda \u003d agent_cfg[\u0027use_gpu\u0027] and torch.cuda.is_available()\n        self.device \u003d torch.device(\"cuda\" if use_cuda else \"cpu\")\n        self.value_net \u003d TorchModel.build(type\u003dagent_cfg[\u0027model\u0027][\u0027type\u0027],\n                                          params\u003dagent_cfg[\u0027model\u0027][\u0027params\u0027])\n        self.target_net \u003d TorchModel.build(type\u003dagent_cfg[\u0027model\u0027][\u0027type\u0027],\n                                           params\u003dagent_cfg[\u0027model\u0027][\u0027params\u0027])\n        self.value_net.to_device(self.device)\n        self.target_net.to_device(self.device)\n        self.target_net.load_state_dict(self.value_net.state_dict())\n        self.target_net.eval()\n        self.optimizer \u003d optim.Adam(self.value_net.parameters(), self.lr)\n\n        # Huber loss acts like the mean squared error when the error is small,\n        # but like the mean absolute error when the error is large\n        # this makes it more robust to outliers when the estimates of Q\n        # are very noisy. It is calculated over a batch of transitions\n        # sampled from the replay memory:\n        self.loss_func \u003d nn.SmoothL1Loss()\n\n    @classmethod\n    def from_params(cls, env: Env, params: Dict):\n        return cls(env, **params)\n\n    @torch.no_grad()\n    def get_action(self, observation) -\u003e int:\n        \"\"\"either take greedy action or explore with epsilon rate\"\"\"\n        if np.random.random() \u003c self.eps:\n            # return self.env.action_space.sample()\n            return np.random.randint(self.env.action_space.n)\n        else:\n            state \u003d torch.FloatTensor(observation).to(self.device)\n            return self.value_net(state).max(1)[1].data[0].item()\n        # observation \u003d torch.FloatTensor(observation).to(self.device)\n        # dist \u003d get_epsilon_dist(eps\u003dself.eps, env\u003dself.env,\n        #                         model\u003dself.value_net, observation\u003dobservation)\n        # return dist.sample().item()\n\n    def train(self):\n        raise NotImplementedError(\u0027DeepTD Agent requires a train() method\u0027)\n\n    def training_update(self):\n        \"\"\"backprop loss, target network, epsilon, and result updates\"\"\"\n        self.epochs +\u003d 1\n        self.train()\n\n        # hard update\n        if self.update_type \u003d\u003d TargetUpdate.HARD.value:\n            if self.epochs !\u003d 0 and self.update_freq % self.epochs \u003d\u003d 0:\n                self.target_net.load_state_dict(\n                    self.value_net.state_dict())\n        elif self.update_type \u003d\u003d TargetUpdate.SOFT.value:\n            soft_update(value_net\u003dself.value_net, target_net\u003dself.target_net,\n                        tau\u003dself.tau)\n\n        if self.use_eps_decay:\n            # self.eps \u003d get_epsilon(eps_start\u003dself.eps, eps_final\u003dself.eps_min,\n            #                        eps_decay\u003dself.eps_decay, t\u003d1)\n            if self.eps \u003e\u003d self.eps_min:\n                self.eps *\u003d self.eps_decay\n\n        # save episodic results\n        self.episodic_result[\u0027Training/Q-Loss\u0027].append(\n            self.loss.detach().cpu().numpy())\n        self.episodic_result[\u0027Training/Mean-Q-Value-Action\u0027].append(\n            np.mean(self.q.detach().cpu().numpy()))\n        self.episodic_result[\n            \u0027Training/Mean-Q-Value-Opposite-Action\u0027].append(\n            np.mean(self.q_.detach().cpu().numpy()))\n        # self.episodic_result[\u0027value_net_params\u0027].append(\n        #     self.value_net.named_parameters())\n\n    def learn(self, num_steps: int) -\u003e Generator:\n        \"\"\"use agent to interact with environment by making actions based on\n        optimal policy to obtain cumulative rewards\"\"\"\n        cr, t \u003d 0, 0\n        done \u003d False\n        start \u003d time.time()\n        observation \u003d self.env.reset()\n        observation \u003d np.expand_dims(observation, 0)\n        action \u003d self.get_action(observation\u003dobservation)\n\n        while not done and t \u003c num_steps:\n            self.total_steps +\u003d 1\n            next_observation, reward, done, info \u003d self.env.step(action)\n            next_observation \u003d np.expand_dims(next_observation, 0)\n            cr +\u003d reward\n\n            # store into experience replay buffer and sample batch of\n            # transitions to estimate the q-values and train on losses\n            transition \u003d Transition(s0\u003dobservation, a\u003daction, r\u003dreward,\n                                    s1\u003dnext_observation, done\u003ddone)\n            self.replay_buffer.push(transition)\n\n            # train policy network and update target network\n            # update epsilon decay, more starting exploration\n            if self.total_steps \u003e\u003d self.warm_up_freq:\n                self.training_update()\n\n            observation \u003d next_observation\n            action \u003d self.get_action(observation\u003dobservation)\n            t +\u003d 1\n\n        # TODO: pause training and use eval with generator, need to add eval!\n        yield {\n            \u0027cum_reward\u0027: cr,\n            \u0027time_to_solve\u0027: t,\n            \u0027mean_q_loss\u0027: self.episodic_result[\u0027Training/Q-Loss\u0027][-t:],\n            \u0027mean_action_q_value\u0027: np.mean(self.episodic_result[\n                                       \u0027Training/Mean-Q-Value-Action\u0027][-t:]),\n            \u0027mean_opposite_action_q_value\u0027: np.mean(self.episodic_result[\n                                       \u0027Training/Mean-Q-Value-Opposite-Action\u0027][\n                                   -t:]),\n            \u0027episode_time\u0027: time.time() - start,\n        }\n\n\n@Agent.register(\u0027DQNAgent\u0027)\nclass DQNAgent(DeepTDAgent, Registrable):\n    def __init__(self, env: Env, agent_cfg: Dict):\n        super().__init__(env, agent_cfg)\n        self.use_double \u003d agent_cfg[\u0027use_double\u0027]\n\n    @classmethod\n    def from_params(cls, env: Env, params: Dict):\n        return cls(env, params)\n\n    def train(self):\n        # handle different replay types\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            batch \u003d self.replay_buffer.sample(batch_size\u003dself.batch_size)\n        elif self.replay_buffer.replay_type \u003d\u003d \\\n                ReplayType.PRIORITIZED_EXPERIENCE_REPLAY.value:\n            batch, weights \u003d self.replay_buffer.sample(\n                batch_size\u003dself.batch_size)\n\n        # handle different DQN\n        if self.use_double:\n            next_q \u003d self.target_net(batch.s1).max(1)[0]\n        else:\n            next_q_actions \u003d torch.max(self.value_net(batch.s1), dim\u003d1)[1]\n            next_q \u003d self.target_net(batch.s1).gather(1,\n                                                      next_q_actions.unsqueeze(\n                                                          1)).squeeze(1)\n\n        # expected Q and Q using value net\n        q_values \u003d self.value_net(batch.s0)\n        self.q \u003d q_values.gather(1, batch.a.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            self.q_ \u003d q_values.gather(1, 1 - batch.a.unsqueeze(1)).squeeze(1)\n            expected_q \u003d batch.r + self.gamma * (1 - batch.done) * next_q\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q)\n        elif self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q) * torch.FloatTensor(\n                weights)\n        # torchviz.make_dot(self.loss).render(\u0027loss\u0027)\n        self.loss \u003d self.loss.mean()\n        self.optimizer.zero_grad()\n        self.loss.backward()\n\n        # gradient clipping to avoid loss divergence based on DeepMind\u0027s DQN in\n        # 2015, where the author clipped the gradient within [-1, 1]\n        if self.use_grad_clipping:\n            nn.utils.clip_grad_norm_(self.value_net.parameters(),\n                                     self.grad_clipping)\n\n        # update replay buffer..\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            # less memory used\n            with torch.no_grad():\n                abs_td_error \u003d torch.abs(expected_q - self.q).cpu().numpy() + \\\n                               self.replay_buffer.non_zero_variant\n            self.replay_buffer.update_priorities(losses\u003dabs_td_error)\n        self.optimizer.step()\n\n\n@Agent.register(\u0027DeepSarsaAgent\u0027)\nclass DeepSarsaAgent(DeepTDAgent, Registrable):\n    def __init__(self, env: Env, agent_cfg: Dict):\n        super().__init__(env, agent_cfg)\n\n    @classmethod\n    def from_params(cls, env: Env, params: Dict):\n        return cls(env, params)\n\n    def train(self):\n        # handle different replay types\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            batch \u003d self.replay_buffer.sample(batch_size\u003dself.batch_size)\n        elif self.replay_buffer.replay_type \u003d\u003d \\\n                ReplayType.PRIORITIZED_EXPERIENCE_REPLAY.value:\n            batch, weights \u003d self.replay_buffer.sample(\n                batch_size\u003dself.batch_size)\n\n        next_q \u003d self.target_net(batch.s1).gather(1,\n                                                  batch.a.unsqueeze(1)).squeeze(\n            1)\n\n        q_values \u003d self.value_net(batch.s0)\n        self.q \u003d q_values.gather(1, batch.a.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            self.q_ \u003d q_values.gather(1, 1 - batch.a.unsqueeze(1)).squeeze(1)\n            expected_q \u003d batch.r + self.gamma * (1 - batch.done) * next_q\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q)\n        elif self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q) * torch.FloatTensor(\n                weights)\n        # torchviz.make_dot(self.loss).render(\u0027loss\u0027)\n        self.loss \u003d self.loss.mean()\n        self.optimizer.zero_grad()\n        self.loss.backward()\n\n        # gradient clipping to avoid loss divergence based on DeepMind\u0027s DQN in\n        # 2015, where the author clipped the gradient within [-1, 1]\n        if self.use_grad_clipping:\n            nn.utils.clip_grad_norm_(self.value_net.parameters(),\n                                     self.grad_clipping)\n\n        # update replay buffer..\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            # less memory used\n            with torch.no_grad():\n                abs_td_error \u003d torch.abs(expected_q - self.q).cpu().numpy() + \\\n                               self.replay_buffer.non_zero_variant\n            self.replay_buffer.update_priorities(losses\u003dabs_td_error)\n        self.optimizer.step()\n\n\n@Agent.register(\u0027DeepExpectedSarsaAgent\u0027)\nclass DeepExpectedSarsaAgent(DeepTDAgent, Registrable):\n    def __init__(self, env: Env, agent_cfg: Dict):\n        super().__init__(env, agent_cfg)\n\n    @classmethod\n    def from_params(cls, env: Env, params: Dict):\n        return cls(env, params)\n\n    def train(self):\n        # handle different replay types\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            batch \u003d self.replay_buffer.sample(batch_size\u003dself.batch_size)\n        elif self.replay_buffer.replay_type \u003d\u003d \\\n                ReplayType.PRIORITIZED_EXPERIENCE_REPLAY.value:\n            batch, weights \u003d self.replay_buffer.sample(\n                batch_size\u003dself.batch_size)\n        prob_dist \u003d get_epsilon_dist(eps\u003dself.eps, env\u003dself.env,\n                                     model\u003dself.value_net, observation\u003dbatch.s1)\n        next_q \u003d torch.sum(self.target_net(batch.s1) * prob_dist.probs,\n                           axis\u003d1)\n\n        # expected Q and Q using value net\n        q_values \u003d self.value_net(batch.s0)\n        self.q \u003d q_values.gather(1, batch.a.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            self.q_ \u003d q_values.gather(1, 1 - batch.a.unsqueeze(1)).squeeze(1)\n            expected_q \u003d batch.r + self.gamma * (1 - batch.done) * next_q\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType.EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q)\n        elif self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            self.loss \u003d self.loss_func(expected_q, self.q) * torch.FloatTensor(\n                weights)\n        # torchviz.make_dot(self.loss).render(\u0027loss\u0027)\n        self.loss \u003d self.loss.mean()\n        self.optimizer.zero_grad()\n        self.loss.backward()\n\n        # gradient clipping to avoid loss divergence based on DeepMind\u0027s DQN in\n        # 2015, where the author clipped the gradient within [-1, 1]\n        if self.use_grad_clipping:\n            nn.utils.clip_grad_norm_(self.value_net.parameters(),\n                                     self.grad_clipping)\n\n        # update replay buffer..\n        if self.replay_buffer.replay_type \u003d\u003d ReplayType. \\\n                PRIORITIZED_EXPERIENCE_REPLAY.value:\n            # less memory used\n            with torch.no_grad():\n                abs_td_error \u003d torch.abs(expected_q - self.q).cpu().numpy() + \\\n                               self.replay_buffer.non_zero_variant\n            self.replay_buffer.update_priorities(losses\u003dabs_td_error)\n        self.optimizer.step()\n\n\nfrom typing import Dict, List\nfrom collections import defaultdict\nfrom datetime import datetime\n\nclass Experiment(Registrable):\n    def __init__(self,\n                 logger: ProjectLogger,\n                 env_names: List,\n                 agents: List,\n                 seeds: List,\n                 experiment_cfg: dict,\n                 agent_cfg: dict,\n                 *args, **kwargs):\n        self.logger \u003d logger\n        self.env_names \u003d env_names\n        self.agents \u003d agents\n        self.seeds \u003d seeds\n        self.experiment_cfg \u003d experiment_cfg\n        self.agent_cfg \u003d agent_cfg\n        self.experiment_cfg[\u0027date\u0027] \u003d datetime.today().strftime(\u0027%Y-%m-%d\u0027)\n\n    @classmethod\n    def build(cls, type: str, logger: ProjectLogger, params: Dict):\n        experiment \u003d cls.by_name(type)\n        return experiment.from_params(logger, params)\n\n    @classmethod\n    def from_params(cls, logger: ProjectLogger, params: Dict):\n        return cls(logger, **params)\n\n    def generate_metrics(self, results: List) -\u003e defaultdict:\n        \"\"\"generate whatever metrics needed for the experiment\"\"\"\n        raise NotImplementedError(\u0027Experiment must generate metrics!\u0027)\n\n\nimport ray\nimport gym\nimport numpy as np\nimport pickle\nfrom collections import defaultdict, deque\nfrom typing import Dict, List, Tuple\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\n@Experiment.register(\u0027DeepTDExperiment\u0027)\nclass DeepTDExperiment(Experiment):\n    def __init__(self,\n                 logger: ProjectLogger,\n                 *args, **kwargs):\n        super().__init__(logger\u003dlogger, *args, **kwargs)\n        self.replay_buffer_capacities \u003d self.experiment_cfg[\n            \u0027replay_buffer_capacities\u0027]\n        self.lrs \u003d self.experiment_cfg[\u0027lrs\u0027]\n\n    @classmethod\n    def from_params(cls, logger: ProjectLogger, params: Dict):\n        return cls(logger, **params)\n\n    def run(self) -\u003e defaultdict:\n        \"\"\"for each gym environment and RL agentrithms, test different replay\n        buffer capaciity over multiple seeds\"\"\"\n        output \u003d defaultdict(nested_d)\n        for env_name in self.env_names:\n            for agent in self.agents:\n                for capacity in self.replay_buffer_capacities:\n                    self.agent_cfg[agent][\u0027experience_replay\u0027][\u0027params\u0027][\n                        \u0027capacity\u0027] \u003d capacity\n                    results \u003d [DeepTDExperiment._inner_run.remote(\n                        agent_cfg\u003dself.agent_cfg,\n                        experiment_cfg\u003dself.experiment_cfg,\n                        env_name\u003denv_name, seed\u003dseed, agent_name\u003dagent) for seed\n                        in self.seeds]\n                    results \u003d ray.get(results)\n                    output \u003d self.generate_metrics(env_name\u003denv_name,\n                                                   agent\u003dagent,\n                                                   capacity\u003dcapacity,\n                                                   results\u003dresults,\n                                                   output\u003doutput)\n                    with open(self.experiment_cfg[\u0027experiment_path\u0027],\n                              \u0027wb\u0027) as file:\n                        pickle.dump(output, file)\n                self.logger.info(\n                    f\u0027Finished running experiments for {env_name} | {agent}\u0027)\n        return output\n\n    @staticmethod\n    @ray.remote\n    def _inner_run(agent_cfg: dict, experiment_cfg: dict,\n                   env_name: str, seed: int \u003d 1, agent_name: str \u003d \u0027sarsa\u0027) -\u003e \\\n            Tuple[np.array, np.array]:\n\n        # seed and result initialization\n        np.random.seed(seed)\n        mean_q_loss \u003d np.zeros((len(experiment_cfg[\u0027lrs\u0027]),\n                                 experiment_cfg[\u0027runs\u0027],\n                                 experiment_cfg[\u0027episodes\u0027]))\n        cum_reward \u003d np.zeros((len(experiment_cfg[\u0027lrs\u0027]),\n                               experiment_cfg[\u0027runs\u0027],\n                               experiment_cfg[\u0027episodes\u0027]))\n        time_to_solve \u003d np.ones((len(experiment_cfg[\u0027lrs\u0027]),\n                                 experiment_cfg[\u0027runs\u0027],\n                                 experiment_cfg[\u0027episodes\u0027])) * experiment_cfg[\n                            \u0027steps\u0027]\n        env \u003d gym.make(env_name)\n\n        # O(lrs * runs * episodes * max(test_rng * steps, steps))\n        for i_lr in range(len(experiment_cfg[\u0027lrs\u0027])):\n            # create agent, set the learning rate, tensorboard path..\n            agent_config \u003d agent_cfg[agent_name]\n            agent_config[\u0027lr\u0027] \u003d experiment_cfg[\u0027lrs\u0027][i_lr]\n            agent_config[\u0027seed\u0027] \u003d seed\n            agent \u003d Agent.build(type\u003dagent_name, env\u003denv, params\u003dagent_config)\n\n            # go through runs, in order to further average, and episodes\n            for r in range(experiment_cfg[\u0027runs\u0027]):\n                for i_episode in range(experiment_cfg[\u0027episodes\u0027]):\n                    generator_obj \u003d agent.learn(\n                        num_steps\u003dexperiment_cfg[\u0027steps\u0027])\n                    episode_result \u003d next(generator_obj)\n                    cum_reward[i_lr, r, i_episode] \u003d episode_result[\n                        \u0027cum_reward\u0027]\n                    time_to_solve[i_lr, r, i_episode] \u003d episode_result[\n                        \u0027time_to_solve\u0027]\n                    mean_q_loss[i_lr, r, i_episode] \u003d episode_result[\n                        \u0027mean_q_loss\u0027]\n                    \n                    msg \u003d f\"lr {agent_config[\u0027lr\u0027]} | run {r} | \" \\\n                        f\"episode {i_episode} | eps {agent.eps} \"\n                    for k, v in episode_result.items():\n                        msg +\u003d f\"| {k} {v} \"\n                    print(msg)\n        env.close()\n\n        # generates learning rates * episodes\n        cum_reward \u003d np.mean(cum_reward, axis\u003d1)\n        time_to_solve \u003d np.mean(time_to_solve, axis\u003d1)\n        return cum_reward, time_to_solve, mean_q_loss\n\n    def generate_metrics(self, env_name: str, agent: str, capacity: int,\n                         results: List, output: defaultdict) -\u003e defaultdict:\n        \"\"\"generate whatever metrics needed for the experiment\"\"\"\n        # results over the seeds\n        for idx, lr in enumerate(self.experiment_cfg[\u0027lrs\u0027]):\n            output[env_name][agent][capacity][\u0027mean_q_loss\u0027][lr] \u003d np.mean(\n                [results[i][2] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027mean_cum_rewards\u0027][lr] \u003d np.mean(\n                [results[i][0] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027std_cum_rewards\u0027][lr] \u003d np.std(\n                [results[i][0] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027upper_std_cum_rewards\u0027][lr] \u003d \\\n                output[env_name][agent][capacity][\u0027mean_cum_rewards\u0027][lr] + \\\n                output[env_name][agent][capacity][\u0027std_cum_rewards\u0027][lr]\n            output[env_name][agent][capacity][\u0027lower_std_cum_rewards\u0027][lr] \u003d \\\n                output[env_name][agent][capacity][\u0027mean_cum_rewards\u0027][lr] - \\\n                output[env_name][agent][capacity][\u0027std_cum_rewards\u0027][lr]\n            output[env_name][agent][capacity][\u0027max_cum_rewards\u0027][lr] \u003d np.max(\n                [results[i][0] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027mean_timesteps\u0027][lr] \u003d np.mean(\n                [results[i][1] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027min_timesteps\u0027][lr] \u003d np.min(\n                [results[i][1] for i in range(len(results))], axis\u003d0)[idx]\n            output[env_name][agent][capacity][\u0027max_timesteps\u0027][lr] \u003d np.max(\n                [results[i][1] for i in range(len(results))], axis\u003d0)[idx]\n        return output\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef plot_lr_reward(output: Dict):\n    for env_name in output.keys():\n        for agent, capacities in output[env_name].items():\n\n            plt.rcParams.update({\u0027font.size\u0027: 18})\n            fig \u003d plt.figure(figsize\u003d(10, 16)).add_subplot(111)\n            fig.title.set_text(f\u0027{agent} Plot #1\u0027)\n            fig.set_ylabel(\u0027Average Reward of last 10 episodes\u0027)\n            fig.set_xlabel(r\u0027$\\alpha$\u0027)\n\n            for capacity, metrics in capacities.items():\n                for metric, lrs in metrics.items():\n                    if metric \u003d\u003d \u0027mean_cum_rewards\u0027:\n                        fig.plot(list(lrs.keys()),\n                                 [np.mean(v[-10:]) for k, v in lrs.items()],\n                                 label\u003df\u0027capacity\u003d{capacity}\u0027)\n\n            plt.grid(linestyle\u003d\u0027--\u0027)\n            plt.legend(loc\u003d\u0027upper left\u0027)\n            plt.show()\n            plt.savefig(f\u0027{agent}_plot_1.png\u0027)\n            plt.clf()\n\n    # find max based on...?\n    max_cap, max_lr \u003d 500, 0.05\n    # pick a capacity/lr with best episode rewards..\n    for env_name in output.keys():\n        for agent, capacities in output[env_name].items():\n            plt.rcParams.update({\u0027font.size\u0027: 18})\n            fig \u003d plt.figure(figsize\u003d(16, 10)).add_subplot(111)\n            fig.title.set_text(f\u0027{agent} Plot #2\u0027)\n            fig.set_ylabel(\u0027Reward\u0027)\n            fig.set_xlabel(\u0027Episode\u0027)\n            for capacity, metrics in capacities.items():\n                if capacity \u003d\u003d max_cap:\n                    for metric, lrs in metrics.items():\n                        if \u0027mean\u0027 in metric or \u0027upper\u0027 in metric or \u0027lower\u0027 in metric:\n                            fig.plot(np.arange(len(lrs[max_lr])),\n                                     lrs[max_lr],\n                                     label\u003dmetric)\n            plt.grid(linestyle\u003d\u0027--\u0027)\n            plt.legend(loc\u003d\u0027upper left\u0027)\n            plt.show()\n            plt.savefig(f\u0027{agent}_plot_2.png\u0027)\n            plt.clf()\n\n\n\n\nif __name__ \u003d\u003d \"__main__\":\n    # logger\n    logger \u003d ProjectLogger(level\u003d10)\n\n    # load initial configs for params\n    cfg \u003d evaluate_file(f\u0027{CONFIG_DIR}/n_step_td_config.jsonnet\u0027)\n    logger.info(f\u0027Using the following config: \\n{cfg}\u0027)\n    cfg \u003d json.loads(cfg)\n    \n    ray.init(\n        # local_mode\u003dTrue,\n        ignore_reinit_error\u003dTrue,\n    )\n\n    # specs for the experiment \u0026 agent\n    experiment_cfg, agent_cfg \u003d cfg[\u0027experiment_cfg\u0027], cfg[\u0027agent_cfg\u0027]\n    experiment_path \u003d f\"{RESULT_DIR}/\" \\\n        f\"{cfg[\u0027experiment_name\u0027]}_experiments.pickle\"\n    hyperparams_path \u003d f\"{RESULT_DIR}/\" \\\n        f\"{cfg[\u0027experiment_name\u0027]}_experiments_hyperparameters.pickle\"\n    tensorboard_path \u003d f\"{TENSORBOARD_DIR}/{cfg[\u0027experiment_name\u0027]}/trainer\"\n    experiment_cfg[\u0027experiment_path\u0027] \u003d experiment_path\n    experiment_cfg[\u0027hyperparams_path\u0027] \u003d hyperparams_path\n    experiment_cfg[\u0027tensorboard_path\u0027] \u003d tensorboard_path\n    seeds \u003d np.random.choice(99999, 10, replace\u003dFalse)\n    # seeds \u003d [1337]\n    agents \u003d cfg[\u0027agents\u0027]\n    env_names \u003d cfg[\u0027env_names\u0027]\n    params \u003d {\u0027env_names\u0027: env_names, \u0027agents\u0027: agents, \u0027seeds\u0027: seeds,\n              \u0027experiment_cfg\u0027: experiment_cfg, \u0027agent_cfg\u0027: agent_cfg}\n\n    experiment \u003d Experiment.build(type\u003dcfg[\u0027experiment_name\u0027], logger\u003dlogger,\n                                  params\u003dparams)\n\n    # run dp experiments\n    output \u003d experiment.run()\n    logger.info(f\u0027Finished running experiments\u0027)\n    \n    # with open(f\u0027{RESULT_DIR}/experiments.pickle\u0027, \u0027rb\u0027) as file:\n    #     output \u003d pickle.load(file)\n    # \n    # cfg \u003d evaluate_file(f\u0027{CONFIG_DIR}/n_step_td_config.jsonnet\u0027)\n    # cfg \u003d json.loads(cfg)\n    # \n    # # specs for the experiment\n    # experiment_cfg \u003d cfg[\u0027experiment_cfg\u0027]\n    # plot_lr_reward(output)"
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}