{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2_6235.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6NbgIi9s-n3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdJ54M4IAY_t",
        "colab_type": "text"
      },
      "source": [
        "# Implement Random Walk"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zr5igmfM_ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJc4oF2eVGin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# make one class so that parameters can be shared\n",
        "class RandomWalk():\n",
        "\n",
        "    def __init__(self,\n",
        "                 lamda = None, # for TD(\\lambda)\n",
        "                 interval = 10, \n",
        "                 start = 0, # start of random walk\n",
        "                 end = 1, # end of random walk\n",
        "                 tile_num = 10, # number of total tilings\n",
        "                 alpha = None, # learning rate for TD update\n",
        "                 discount = 1, # discount for TD update\n",
        "                 termination = False # indicator if the episode ends\n",
        "                 ):\n",
        "        self.interval = interval\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.tile_num = tile_num\n",
        "        self.offset = np.zeros(self.tile_num) # offset for tilings\n",
        "        self.tiling = np.tile(np.linspace(self.start, self.end, 1+1+self.interval), (self.tile_num, 1)) # to help determine feature vector\n",
        "        self.w = np.zeros(self.tile_num * (1+self.interval))\n",
        "        self.alpha = alpha # for TD\n",
        "\n",
        "        self.lamda = lamda\n",
        "        self.trace = np.zeros(self.tile_num * (1+self.interval)) #eligibility trace is same shape with self.w\n",
        "        self.gamma = discount\n",
        "        self.termination = termination\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "    def make_tiling(self):\n",
        "\n",
        "        self.tiling = np.tile(np.linspace(self.start, self.end, 1+1+self.interval), (self.tile_num, 1))\n",
        "        self.offset = np.random.uniform(low = 0, high = (self.end-self.start)/self.interval, \n",
        "                                        size = self.tile_num)\n",
        "        self.tiling += self.offset.reshape(self.tile_num, -1) - (self.end-self.start)/self.interval\n",
        "\n",
        "    # approximate value using tile coding\n",
        "    def appr_value(self, loc):\n",
        "        index = np.sum(self.tiling < loc, axis = 1) -1\n",
        "        feature = np.zeros((self.tile_num, 1+self.interval))\n",
        "        feature[:, index] = 1\n",
        "        #return np.sum(self.w[feature==1]) \n",
        "        return np.matmul(feature.flatten().reshape((1, -1)), self.w)\n",
        "\n",
        "\n",
        "        # change to new state\n",
        "        # given current state, loc\n",
        "        # need to specify seed\n",
        "    def next_step(self, loc):\n",
        "        dist = np.random.uniform(low = -0.2, high = 0.2, size = 1)\n",
        "        next_loc = loc + dist\n",
        "        if next_loc < 0 or next_loc > 1:\n",
        "            reward = next_loc # episode terminates\n",
        "            self.termination = True\n",
        "        else:\n",
        "            reward = 0\n",
        "        return reward, next_loc\n",
        "\n",
        "    # update state values and trace values\n",
        "    def update(self, loc, reward, next_loc):\n",
        "        curr_value = self.appr_value(loc)\n",
        "        next_value = self.appr_value(next_loc)\n",
        "        delta = reward + self.gamma * next_value - curr_value\n",
        "        self.trace = self.gamma * self.lamda * self.trace + self.w\n",
        "        self.w += self.alpha * delta * self.trace\n",
        "\n",
        "\n",
        "    # random walk process, loop until self.terminate\n",
        "    def walk(self, loc = 0.5):\n",
        "        # need to re-initiated for each episode\n",
        "        while not self.termination:\n",
        "            reward, next_loc = self.next_step(loc)\n",
        "            self.update(loc, reward, next_loc)\n",
        "\n",
        "    def train_evaluate(self, episodes = 25):\n",
        "        loss = 0\n",
        "        self.make_tiling()\n",
        "        for i in range(episodes):\n",
        "            self.trace = np.zeros(self.tile_num * (1+self.interval))\n",
        "            self.termination = False\n",
        "            self.walk(loc = 0.5)\n",
        "        #return self.w\n",
        "\n",
        "        # evaluate stage\n",
        "        points = np.linspace(0, 1, 21)\n",
        "        for point in points:\n",
        "            loss += (point - self.appr_value(point))**2\n",
        "        return loss/21\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ponvx3ZcMeRe",
        "colab_type": "text"
      },
      "source": [
        "# Plot Experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fp_R3IJCEBk0",
        "colab_type": "code",
        "outputId": "5d8e4f3c-7e9f-4bcc-fdbe-9108e60fc7ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "\n",
        "plt.rcParams.update({'font.size': 22})\n",
        "fig = plt.figure(figsize=(10, 8)).add_subplot(111)\n",
        "fig.title.set_text(r'TD($\\lambda$) With Linear Function Approximation')\n",
        "fig.set_ylabel('Mean Squred Value Error')\n",
        "fig.set_xlabel(r'$\\alpha$')\n",
        "\n",
        "\n",
        "alpha_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
        "lamda_list = [0, 0.4, 0.8, 0.9, 0.95, 0.975, 0.99, 1]\n",
        "\n",
        "runs = 50\n",
        "for lamda in lamda_list:\n",
        "    print(f'lambda value is {lamda}')\n",
        "    loss = np.zeros((len(alpha_list), runs))\n",
        "    for i in range(len(alpha_list)):\n",
        "        print(f'\\t\\t alpha value is {alpha_list[i]}')\n",
        "        for run in range(runs):\n",
        "            np.random.seed(run)\n",
        "            experiment = RandomWalk(lamda = lamda, alpha = alpha_list[i])\n",
        "            result = experiment.train_evaluate()\n",
        "            loss[i, run] = result\n",
        "    fig.plot(alpha_list, np.mean(loss, axis = 1), label = r'$\\lambda$' + '=' + str(lamda))\n",
        "        \n",
        "\n",
        "plt.legend(loc = 'upper right')\n",
        "plt.savefig('TD.png')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "lambda value is 0\n",
            "\t\t alpha value is 0.1\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}